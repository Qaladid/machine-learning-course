{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIT4PV9YUiICF/WS6532+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qaladid/machine-learning-course/blob/main/homework_serverless.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruUc8oocBiIG",
        "outputId": "d509100c-5d6c-4d8c-98fc-2583f5551655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "# Define the URL of the model\n",
        "model_url = 'https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/model_2024_hairstyle.keras'\n",
        "model_path = 'model_2024_hairstyle.keras'\n",
        "\n",
        "# Download the model file\n",
        "urllib.request.urlretrieve(model_url, model_path)\n",
        "print(\"Model downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Keras model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAXm8J-nBw8K",
        "outputId": "0907fd09-bae2-4ff3-9882-7a991b93a3c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model as a .tflite file\n",
        "tflite_model_path = 'model_2024_hairstyle.tflite'\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model converted to TF Lite format and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEXATOqsCCqx",
        "outputId": "ef8b34c0-c914-4316-fb08-781441a17720"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp2x74be9r'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140435729463280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140435728419824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140435728430384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140435727911568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140435727914208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140435728212992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model converted to TF Lite format and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the size of the converted model in MB\n",
        "model_size = os.path.getsize(tflite_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"Size of the converted TF Lite model: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j78-jv3wCMjs",
        "outputId": "ca3abe97-91bd-4b39-9c2a-073fbc8f453f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the converted TF Lite model: 76.58 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model_2024_hairstyle.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get the input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Print the output details to check the index\n",
        "print(\"Input details:\", input_details)\n",
        "print(\"Output details:\", output_details)\n",
        "\n",
        "# Get the index of the output\n",
        "output_index = output_details[0]['index']\n",
        "print(f\"Output index: {output_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg96lme9Cav_",
        "outputId": "b548b279-8701-470d-eb93-1f53b15694ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input details: [{'name': 'serving_default_input_layer:0', 'index': 0, 'shape': array([  1, 200, 200,   3], dtype=int32), 'shape_signature': array([ -1, 200, 200,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 13, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output index: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from urllib import request\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Download the image\n",
        "def download_image(url):\n",
        "    with request.urlopen(url) as resp:\n",
        "        buffer = resp.read()\n",
        "    stream = BytesIO(buffer)\n",
        "    img = Image.open(stream)\n",
        "    return img\n",
        "\n",
        "# Prepare the image: resize and ensure RGB format\n",
        "def prepare_image(img, target_size):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')  # Convert to RGB if not already\n",
        "    img = img.resize(target_size, Image.NEAREST)  # Resize the image to target size\n",
        "    return img\n",
        "\n",
        "# URL of the image\n",
        "image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
        "\n",
        "# Download and prepare the image\n",
        "img = download_image(image_url)\n",
        "\n",
        "# Target size (we need to determine this)"
      ],
      "metadata": {
        "id": "WcEViOjEDZZi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the target size is (200, 200)\n",
        "target_size = (200, 200)\n",
        "img_resized = prepare_image(img, target_size)"
      ],
      "metadata": {
        "id": "h5C0qfYiEJ4k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to numpy array\n",
        "img_array = np.array(img_resized)\n",
        "\n",
        "# Normalize pixel values (if the model expects a range between 0 and 1)\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Check the shape of the array\n",
        "print(img_array.shape)  # This should be (200, 200, 3) if the target size is (200, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIfkvM2PEsLj",
        "outputId": "fcb9e39e-5659-41e9-ae00-696da06b5f10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 200, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Red channel value for the first pixel (at position [0, 0])\n",
        "red_channel_value = img_array[0, 0, 0]  # [0, 0] is the first pixel, 0 is for the Red channel\n",
        "\n",
        "print(f\"Red channel value of the first pixel: {red_channel_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq469hRSEvt3",
        "outputId": "3d0e2b8b-ee20-4be8-c1c0-55e73dd6e29c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red channel value of the first pixel: 0.23921568627450981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zekk86QhEy6r",
        "outputId": "ec9b7763-1b7c-4e5c-cb96-66ae9ca5a5c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflite-runtime\n",
            "  Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite-runtime) (1.26.4)\n",
            "Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "model_path = '/content/model_2024_hairstyle.tflite'  # Update this with the correct path\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "# Allocate tensors\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare the image: Ensure it is in the correct shape\n",
        "# Assuming the target size is (200, 200)\n",
        "img_resized = prepare_image(img, (200, 200))\n",
        "\n",
        "# Convert the image to a numpy array and normalize it (same steps as before)\n",
        "img_array = np.array(img_resized) / 255.0\n",
        "\n",
        "# Reshape to match the model input (if necessary, sometimes it requires adding a batch dimension)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Shape becomes (1, 200, 200, 3)\n",
        "\n",
        "# Set the input tensor\n",
        "input_tensor_index = input_details[0]['index']\n",
        "interpreter.set_tensor(input_tensor_index, img_array.astype(np.float32))\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_tensor_index = output_details[0]['index']\n",
        "output = interpreter.get_tensor(output_tensor_index)\n",
        "\n",
        "# Print the output prediction\n",
        "print(\"Model Output:\", output[0][0])  # The model returns the prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE2di1-BFWLV",
        "outputId": "9ee8af77-deb8-47c0-f632-66b3cecd315f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: 0.8937741\n"
          ]
        }
      ]
    }
  ]
}